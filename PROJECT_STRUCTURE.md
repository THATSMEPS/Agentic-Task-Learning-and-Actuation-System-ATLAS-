# ATLAS Project Structure

```
The Atlas(Agentic Task Learning and Actuation System) Robot/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      # Main project documentation
â”œâ”€â”€ ğŸ“„ QUICK_START.md                 # Quick start testing guide
â”œâ”€â”€ ğŸ“„ IMPLEMENTATION_SUMMARY.md      # Detailed implementation summary
â”‚
â”œâ”€â”€ ğŸš€ main.py                        # Main entry point - START HERE
â”‚
â”œâ”€â”€ ğŸ¤– Core Robot Modules
â”‚   â”œâ”€â”€ agent.py                      # Main orchestration & state machine
â”‚   â”œâ”€â”€ vision.py                     # Computer vision & object detection
â”‚   â”œâ”€â”€ navigation.py                 # Path planning & visual servoing
â”‚   â”œâ”€â”€ motor_control.py              # 4-wheel drive control
â”‚   â”œâ”€â”€ arm_control.py                # Robotic arm control
â”‚   â”œâ”€â”€ llm_interface.py              # Natural language processing (Gemini)
â”‚   â””â”€â”€ speech_interface.py           # Voice input/output
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ config.py                     # Main configuration (original)
â”‚   â”œâ”€â”€ config_extended.py            # Extended configuration options
â”‚   â””â”€â”€ requirements.txt              # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ test_videos/                   # Optional test videos for vision
â”‚
â””â”€â”€ ğŸ”§ Development Files
    â”œâ”€â”€ .env                          # Environment variables (API keys)
    â”œâ”€â”€ .gitignore                    # Git ignore rules
    â”œâ”€â”€ .venv/                        # Python virtual environment
    â””â”€â”€ .git/                         # Git repository
```

---

## ğŸ“„ File Descriptions

### Main Entry Points

| File | Purpose | Run Command |
|------|---------|-------------|
| `main.py` | Start the complete ATLAS system | `python main.py` |
| `agent.py` | Can also run directly | `python agent.py` |

### Core Modules (Can Test Individually)

| Module | Purpose | Test Command | What It Does |
|--------|---------|--------------|--------------|
| `vision.py` | Computer vision | `python vision.py` | Detect objects with laptop camera |
| `motor_control.py` | Mobility | `python motor_control.py` | Simulate robot movement |
| `arm_control.py` | Manipulation | `python arm_control.py` | Simulate arm movements |
| `navigation.py` | Path planning | `python navigation.py` | Test navigation algorithms |
| `llm_interface.py` | NLP | `python llm_interface.py` | Test command understanding |
| `speech_interface.py` | Voice I/O | `python speech_interface.py` | Test voice interface |

### Documentation

| File | Contents |
|------|----------|
| `README.md` | Project overview, features, setup instructions |
| `QUICK_START.md` | Step-by-step testing guide |
| `IMPLEMENTATION_SUMMARY.md` | Detailed technical documentation |

### Configuration

| File | Purpose |
|------|---------|
| `config.py` | Basic configuration (camera, colors) |
| `config_extended.py` | Extended parameters (speeds, thresholds) |
| `requirements.txt` | Python package dependencies |
| `.env` | API keys (create this file) |

---

## ğŸ¯ Quick Navigation

### Want to...

**Test the complete system?**
â†’ `python main.py`

**Test vision with your camera?**
â†’ `python vision.py`

**Understand how it works?**
â†’ Read `IMPLEMENTATION_SUMMARY.md`

**Get started quickly?**
â†’ Read `QUICK_START.md`

**Learn about the project?**
â†’ Read `README.md`

**Test individual components?**
â†’ Run any module file directly

---

## ğŸ“Š Module Dependencies

```
main.py
  â””â”€â†’ agent.py
       â”œâ”€â†’ vision.py
       â”‚    â””â”€â†’ config.py
       â”‚    â””â”€â†’ opencv, numpy
       â”‚
       â”œâ”€â†’ navigation.py
       â”‚    â””â”€â†’ vision.py (for visual servoing)
       â”‚
       â”œâ”€â†’ motor_control.py
       â”‚
       â”œâ”€â†’ arm_control.py
       â”‚
       â”œâ”€â†’ llm_interface.py
       â”‚    â””â”€â†’ config.py (for API key)
       â”‚    â””â”€â†’ google.generativeai
       â”‚
       â””â”€â†’ speech_interface.py
```

---

## ğŸ”„ Data Flow

```
User Command (Voice/Text)
    â†“
[LLM Interface] â†’ Parse command
    â†“
[Agent: PLANNING] â†’ Create task plan
    â†“
[Vision] â†’ Set target object
    â†“
[Agent: SEARCHING] â†’ Lawnmower pattern
    â†“
[Motor Control] â†’ Navigate waypoints
    â†“
[Vision] â†’ Detect object
    â†“
[Agent: APPROACHING] â†’ Visual servoing
    â†“
[Navigation: Visual Servo] â†’ Center & approach
    â†“
[Motor Control] â†’ Execute movements
    â†“
[Agent: GRASPING] â†’ Grab object
    â†“
[Arm Control] â†’ Execute grab sequence
    â†“
[Agent: RETURNING] â†’ Navigate home
    â†“
[Motor Control] â†’ Follow breadcrumbs
    â†“
[Arm Control] â†’ Present object
    â†“
[Agent: TASK_COMPLETE] â†’ Release & reset
```

---

## ğŸ§ª Testing Order (Recommended)

1. **Vision System** (`python vision.py`)
   - Most important!
   - Validates camera and object detection
   - Tests with real hardware (your webcam)

2. **LLM Interface** (`python llm_interface.py`)
   - Tests command understanding
   - Can work without API key

3. **Motor Control** (`python motor_control.py`)
   - Simulates movement
   - Tests navigation logic

4. **Arm Control** (`python arm_control.py`)
   - Simulates manipulation
   - Tests grab sequences

5. **Navigation** (`python navigation.py`)
   - Tests path planning
   - Visual servoing calculations

6. **Complete System** (`python main.py`)
   - Full integration test
   - Complete fetch-and-deliver pipeline

---

## ğŸ’¾ File Sizes (Approximate)

| File | Lines | Size |
|------|-------|------|
| `agent.py` | ~330 | 12 KB |
| `vision.py` | ~300 | 11 KB |
| `motor_control.py` | ~300 | 11 KB |
| `arm_control.py` | ~280 | 10 KB |
| `navigation.py` | ~240 | 9 KB |
| `llm_interface.py` | ~150 | 6 KB |
| `speech_interface.py` | ~250 | 9 KB |
| `main.py` | ~40 | 1.5 KB |
| **TOTAL CODE** | ~1,890 | ~70 KB |

---

## ğŸ¨ Code Style

All modules follow consistent patterns:

```python
# module_name.py
"""
Module docstring explaining purpose
"""

# Imports
import standard_lib
import third_party
import local_modules

# Classes
class MainClass:
    """Class docstring"""
    
    def __init__(self):
        """Initialize with hardware simulation"""
        self.hardware_connected = False
        print("[MODULE] - Initialized")
    
    def initialize_hardware(self):
        """Setup real hardware"""
        # TODO: Uncomment when hardware is connected
        pass
    
    def main_functionality(self):
        """Core functionality"""
        print("[MOCK HARDWARE] - What hardware would do")
        # Real logic here
        
# Test harness
if __name__ == "__main__":
    print("=== Testing Module ===")
    # Test code
```

---

## ğŸ·ï¸ Naming Conventions

### Module Names
- Lowercase with underscores: `motor_control.py`

### Class Names
- PascalCase: `MotorController`, `VisionSystem`

### Function/Method Names
- Lowercase with underscores: `find_target_object()`, `navigate_to_point()`

### Constants
- Uppercase with underscores: `FRAME_WIDTH`, `BASE_SPEED`

### State Enum
- PascalCase: `AgentState.SEARCHING`

---

## ğŸ” Configuration Files

### `.env` (Create this)
```bash
GEMINI_API_KEY=your_api_key_here
```

### `config.py` (Existing)
- Camera settings
- Color ranges (legacy)
- Frame dimensions

### `config_extended.py` (New)
- All additional parameters
- Motor speeds
- Servo limits
- Thresholds

---

## ğŸ“¦ Dependencies

### Required (Installed)
- `opencv-python` - Computer vision
- `numpy` - Numerical operations
- `google-generativeai` - LLM integration

### Optional (Commented in requirements.txt)
- `SpeechRecognition` - Voice input
- `pyttsx3` - Text-to-speech
- `pyaudio` - Audio I/O
- `RPi.GPIO` - Raspberry Pi GPIO (hardware)
- `Adafruit-PCA9685` - Servo controller (hardware)

---

## ğŸ¯ Entry Points Summary

```bash
# Main application
python main.py                  # Text input mode
python main.py --voice          # Voice input mode (simulated)

# Individual module tests
python vision.py                # Test vision with webcam
python motor_control.py         # Test motor simulation
python arm_control.py           # Test arm simulation
python navigation.py            # Test navigation algorithms
python llm_interface.py         # Test command parsing
python speech_interface.py      # Test voice interface

# Python interactive
python
>>> from agent import Agent
>>> robot = Agent(use_laptop_camera=True)
>>> # Manual control
```

---

## ğŸ“ Log Output Format

All modules use consistent logging:

```
[MODULE_NAME] - Message type: Details
```

Examples:
```
[AGENT] - ATLAS is online and awaiting commands.
[VISION] - Target set: 'blue phone' with primary color 'blue'
[MOTOR] - Navigating from [0, 0] to (2.5, 1.5)
[ARM] - Moving to 'grab' pose
[NAV] - Generated 14 waypoints for lawnmower search
[LLM] - Plan received: {'action': 'fetch', ...}
[VISUAL_SERVO] - Error: +120px | Distance: 85.3cm
[MOCK HARDWARE] - TURNING RIGHT 12.0Â°
```

This makes it easy to track which subsystem is active!

---

## ğŸš€ Version History

### v1.0 - Complete Implementation
- âœ… All 8 modules implemented
- âœ… Hardware simulation mode working
- âœ… Laptop camera integration
- âœ… Dynamic object detection
- âœ… Visual servoing
- âœ… Distance estimation
- âœ… Complete fetch-and-deliver pipeline
- âœ… LLM integration with fallback
- âœ… Voice interface structure
- âœ… Comprehensive documentation

---

## ğŸ“ Support

**Having issues?**

1. Check `QUICK_START.md` for testing steps
2. Run individual modules to isolate problem
3. Check log output for error messages
4. Verify camera is working (`python vision.py`)
5. Check that all dependencies are installed

**Common Issues:**

| Problem | Solution | File |
|---------|----------|------|
| Camera not found | Change `CAMERA_INDEX` in config | `config.py` |
| Object not detected | Adjust lighting, try red object first | `vision.py` |
| API key error | Set environment variable or use fallback | `llm_interface.py` |
| Import error | Install requirements: `pip install -r requirements.txt` | N/A |

---

**Last Updated:** November 2, 2025
**Status:** âœ… Complete and Ready for Testing
**Hardware:** Currently in simulation mode
